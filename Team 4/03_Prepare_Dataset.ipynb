{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert text to BERT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = \"team-4-project-data\"\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-18 20:23:31 211125778552personalizepocvod\n",
      "2024-02-18 01:27:18 aws-athena-query-results-211125778552-us-east-1\n",
      "2024-03-27 00:35:44 aws-glue-assets-211125778552-us-east-1\n",
      "2024-03-27 00:28:12 aws-glue-assets-211125778552-us-east-2\n",
      "2024-02-15 21:19:44 sagemaker-studio-12jvao34qlkn\n",
      "2024-02-15 22:38:05 sagemaker-studio-211125778552-3pjkfc2ijfr\n",
      "2024-02-17 02:02:09 sagemaker-studio-211125778552-4dcj21sopi3\n",
      "2024-02-19 03:02:29 sagemaker-studio-211125778552-4rfwbx1bibn\n",
      "2024-02-15 20:23:46 sagemaker-studio-211125778552-4yhhjbuzjdq\n",
      "2024-02-17 02:02:35 sagemaker-studio-211125778552-8xxlet4bnrv\n",
      "2024-02-17 02:02:08 sagemaker-studio-211125778552-rfcwvtinree\n",
      "2024-02-20 00:38:45 sagemaker-studio-211125778552-yu1t8p5304s\n",
      "2024-03-21 19:20:47 sagemaker-studio-uyd2sz3oy3\n",
      "2024-03-08 03:11:27 sagemaker-team11-stanford-dogs\n",
      "2024-03-18 01:49:41 sagemaker-team6-distracted-drivers\n",
      "2024-02-15 20:23:48 sagemaker-us-east-1-211125778552\n",
      "2024-03-03 20:33:04 team-3-project-data\n",
      "2024-04-12 00:29:36 team-4-project-data\n",
      "2024-03-20 23:58:20 team-8-project-data\n",
      "2024-03-28 22:10:03 team5bucket\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE athena/\n",
      "                           PRE ryanair-data/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls team-4-project-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE parquet/\n",
      "2024-04-12 00:29:37    1908135 ryanair_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://team-4-project-data/ryanair-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://team-4-project-data/ryanair-data/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = \"s3://{}/ryanair-data/\".format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://team-4-project-data/athena/staging\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_athena_uri = \"s3://{}/athena/staging\".format(bucket)\n",
    "print(raw_input_data_athena_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\n",
      "\u001b[37m# This is 2.3.0 (vs. 2.3.1 everywhere else) because we need to\u001b[39;49;00m\n",
      "\u001b[37m# use anaconda and anaconda only supports 2.3.0 at this time\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"conda\", \"install\", \"-c\", \"anaconda\", \"tensorflow==2.3.0\", \"-y\"])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow==2.3.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.24.1\"])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[37m# import sagemaker\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.session import Session\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.feature_store.feature_group import FeatureGroup\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.feature_store.feature_definition import (\u001b[39;49;00m\n",
      "\u001b[37m#     FeatureDefinition,\u001b[39;49;00m\n",
      "\u001b[37m#     FeatureTypeEnum,\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# region = os.environ[\"AWS_DEFAULT_REGION\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"Region: {}\".format(region))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# #############################\u001b[39;49;00m\n",
      "\u001b[37m# ## We may need to get the Role and Bucket before setting sm, featurestore_runtime, etc.\u001b[39;49;00m\n",
      "\u001b[37m# ## Role and Bucket are malformed if we do this later.\u001b[39;49;00m\n",
      "\u001b[37m# sts = boto3.Session(region_name=region).client(service_name=\"sts\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# caller_identity = sts.get_caller_identity()\u001b[39;49;00m\n",
      "\u001b[37m# print(\"caller_identity: {}\".format(caller_identity))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# assumed_role_arn = caller_identity[\"Arn\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"(assumed_role) caller_identity_arn: {}\".format(assumed_role_arn))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# assumed_role_name = assumed_role_arn.split(\"/\")[-2]\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# iam = boto3.Session(region_name=region).client(service_name=\"iam\", region_name=region)\u001b[39;49;00m\n",
      "\u001b[37m# get_role_response = iam.get_role(RoleName=assumed_role_name)\u001b[39;49;00m\n",
      "\u001b[37m# print(\"get_role_response {}\".format(get_role_response))\u001b[39;49;00m\n",
      "\u001b[37m# role = get_role_response[\"Role\"][\"Arn\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"role {}\".format(role))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# bucket = sagemaker.Session().default_bucket()\u001b[39;49;00m\n",
      "\u001b[37m# print(\"The DEFAULT BUCKET is {}\".format(bucket))\u001b[39;49;00m\n",
      "\u001b[37m# #############################\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# sm = boto3.Session(region_name=region).client(service_name=\"sagemaker\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# featurestore_runtime = boto3.Session(region_name=region).client(\u001b[39;49;00m\n",
      "\u001b[37m#     service_name=\"sagemaker-featurestore-runtime\", region_name=region\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# s3 = boto3.Session(region_name=region).client(service_name=\"s3\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# sagemaker_session = sagemaker.Session(\u001b[39;49;00m\n",
      "\u001b[37m#     boto_session=boto3.Session(region_name=region),\u001b[39;49;00m\n",
      "\u001b[37m#     sagemaker_client=sm,\u001b[39;49;00m\n",
      "\u001b[37m#     sagemaker_featurestore_runtime_client=featurestore_runtime,\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "REVIEW_BODY_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mComment\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "REVIEW_ID_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mRecord ID\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\n",
      "\n",
      "LABEL_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mOverall Rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m, \u001b[34m6\u001b[39;49;00m, \u001b[34m7\u001b[39;49;00m, \u001b[34m8\u001b[39;49;00m, \u001b[34m9\u001b[39;49;00m, \u001b[34m10\u001b[39;49;00m]\n",
      "\n",
      "label_map = {}\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\n",
      "    label_map[label] = i\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m\"\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\n",
      "\n",
      "\n",
      "\u001b[37m# def wait_for_feature_group_creation_complete(feature_group):\u001b[39;49;00m\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         status = feature_group.describe().get(\"FeatureGroupStatus\")\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#         while status == \"Creating\":\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Waiting for Feature Group Creation\")\u001b[39;49;00m\n",
      "\u001b[37m#             time.sleep(5)\u001b[39;49;00m\n",
      "\u001b[37m#             status = feature_group.describe().get(\"FeatureGroupStatus\")\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#         if status != \"Created\":\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#             raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\u001b[39;49;00m\n",
      "\u001b[37m#         print(f\"FeatureGroup {feature_group.name} successfully created.\")\u001b[39;49;00m\n",
      "\u001b[37m#     except:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"No feature group created yet.\")\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# def create_or_load_feature_group(prefix, feature_group_name):\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     # Feature Definitions for our records\u001b[39;49;00m\n",
      "\u001b[37m#     feature_definitions = [\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"input_ids\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"input_mask\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"segment_ids\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"label_id\", feature_type=FeatureTypeEnum.INTEGRAL),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"review_id\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"date\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"label\", feature_type=FeatureTypeEnum.INTEGRAL),\u001b[39;49;00m\n",
      "\u001b[37m#         #        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"split_type\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#     ]\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     feature_group = FeatureGroup(\u001b[39;49;00m\n",
      "\u001b[37m#         name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sagemaker_session\u001b[39;49;00m\n",
      "\u001b[37m#     )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Feature Group: {}\".format(feature_group))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\u001b[39;49;00m\n",
      "\u001b[37m#             \"Waiting for existing Feature Group to become available if it is being created by another instance in our cluster...\"\u001b[39;49;00m\n",
      "\u001b[37m#         )\u001b[39;49;00m\n",
      "\u001b[37m#         wait_for_feature_group_creation_complete(feature_group)\u001b[39;49;00m\n",
      "\u001b[37m#     except Exception as e:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Before CREATE FG wait exeption: {}\".format(e))\u001b[39;49;00m\n",
      "\u001b[37m#     #        pass\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         record_identifier_feature_name = \"review_id\"\u001b[39;49;00m\n",
      "\u001b[37m#         event_time_feature_name = \"date\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         print(\"Creating Feature Group with role {}...\".format(role))\u001b[39;49;00m\n",
      "\u001b[37m#         feature_group.create(\u001b[39;49;00m\n",
      "\u001b[37m#             s3_uri=f\"s3://{bucket}/{prefix}\",\u001b[39;49;00m\n",
      "\u001b[37m#             record_identifier_name=record_identifier_feature_name,\u001b[39;49;00m\n",
      "\u001b[37m#             event_time_feature_name=event_time_feature_name,\u001b[39;49;00m\n",
      "\u001b[37m#             role_arn=role,\u001b[39;49;00m\n",
      "\u001b[37m#             enable_online_store=False,\u001b[39;49;00m\n",
      "\u001b[37m#         )\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Creating Feature Group. Completed.\")\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         print(\"Waiting for new Feature Group to become available...\")\u001b[39;49;00m\n",
      "\u001b[37m#         wait_for_feature_group_creation_complete(feature_group)\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Feature Group available.\")\u001b[39;49;00m\n",
      "\u001b[37m#         feature_group.describe()\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     except Exception as e:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Exception: {}\".format(e))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     return feature_group\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
      "        \u001b[37m#  review_body):\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m          text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\n",
      "\u001b[33m            sequence tasks, only this sequence must be specified.\u001b[39;49;00m\n",
      "\u001b[33m          label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\n",
      "\u001b[33m            specified for train and dev examples, but not for test examples.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.text = text\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    tokens = tokenizer.tokenize(the_input.text)\n",
      "\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    encode_plus_tokens = tokenizer.encode_plus(\n",
      "        the_input.text,\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        max_length=max_seq_length,\n",
      "        truncation=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\n",
      "\n",
      "    \u001b[37m# Label for each training row (`Overall Rating` 1 through 5)\u001b[39;49;00m\n",
      "    label_id = label_map[the_input.label]\n",
      "\n",
      "    features = InputFeatures(\n",
      "        input_ids=input_ids,\n",
      "        input_mask=input_mask,\n",
      "        segment_ids=segment_ids,\n",
      "        label_id=label_id,\n",
      "        review_id=the_input.review_id,\n",
      "        date=the_input.date,\n",
      "        label=the_input.label,\n",
      "    )\n",
      "    \u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\n",
      "    \u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs, output_file, max_seq_length):\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    records = []\n",
      "\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\n",
      "\n",
      "        features = convert_input(the_input, max_seq_length)\n",
      "\n",
      "        all_features = collections.OrderedDict()\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
      "\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\n",
      "\n",
      "        records.append(\n",
      "            {  \u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_mask,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.segment_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.review_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.date,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label,\n",
      "                \u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\n",
      "            }\n",
      "        )\n",
      "\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m# break\u001b[39;49;00m\n",
      "\n",
      "    tf_record_writer.close()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m records\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m  \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.70\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.15\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.15\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, max_seq_length, balance_dataset, prefix, feature_group_name):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(file))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(max_seq_length))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(balance_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(prefix))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(feature_group_name))\n",
      "\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\n",
      "\u001b[37m#    feature_group = create_or_load_feature_group(prefix, feature_group_name)\u001b[39;49;00m\n",
      "\n",
      "    filename_without_extension = Path(Path(file).stem).stem\n",
      "\n",
      "    df = pd.read_csv(file, delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m#df.isna().values.any()\u001b[39;49;00m\n",
      "    \u001b[37m#df = df.dropna()\u001b[39;49;00m\n",
      "    \u001b[37m#df = df.reset_index(drop=True)\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\n",
      "        df_grouped_by = df.groupby([\u001b[33m\"\u001b[39;49;00m\u001b[33mOverall Rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \n",
      "        df_balanced = df_grouped_by.apply(\u001b[34mlambda\u001b[39;49;00m x: x.sample(df_grouped_by.size().min()).reset_index(drop=\u001b[34mTrue\u001b[39;49;00m))\n",
      "\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_balanced.shape))\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m\"\u001b[39;49;00m\u001b[33mOverall Rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\n",
      "\n",
      "        df = df_balanced\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.train_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.validation_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.test_split_percentage))\n",
      "\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(holdout_percentage))\n",
      "    \n",
      "    df_train, df_holdout = train_test_split(df, test_size=holdout_percentage, stratify=df[\u001b[33m\"\u001b[39;49;00m\u001b[33mOverall Rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_holdout_percentage))\n",
      "    \n",
      "    df_validation, df_test = train_test_split(\n",
      "        df_holdout, test_size=test_holdout_percentage, stratify=df_holdout[\u001b[33m\"\u001b[39;49;00m\u001b[33mOverall Rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_validation.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_test.shape))\n",
      "\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\n",
      "\n",
      "    train_inputs = df_train.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    validation_inputs = df_validation.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    test_inputs = df_test.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\n",
      "    train_records = transform_inputs_to_tfrecord(\n",
      "        train_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    validation_records = transform_inputs_to_tfrecord(\n",
      "        validation_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    test_records = transform_inputs_to_tfrecord(\n",
      "        test_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
      "    df_train_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_train_records.head()\n",
      "\n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
      "    df_validation_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_validation_records.head()\n",
      "\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\n",
      "    df_test_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_test_records.head()\n",
      "\n",
      "    \u001b[37m# Add record to feature store\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_train_records = cast_object_to_string(df_train_records)\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_validation_records = cast_object_to_string(df_validation_records)\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_test_records = cast_object_to_string(df_test_records)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Ingesting features...\")\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_train_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_validation_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_test_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "    \n",
      "\u001b[37m#     offline_store_status = None\u001b[39;49;00m\n",
      "\u001b[37m#     while offline_store_status != 'Active':\u001b[39;49;00m\n",
      "\u001b[37m#         try:\u001b[39;49;00m\n",
      "\u001b[37m#             offline_store_status = feature_group.describe()['OfflineStoreStatus']['Status']\u001b[39;49;00m\n",
      "\u001b[37m#         except:\u001b[39;49;00m\n",
      "\u001b[37m#             pass\u001b[39;49;00m\n",
      "\u001b[37m#         print('Offline store status: {}'.format(offline_store_status))    \u001b[39;49;00m\n",
      "\u001b[37m#     print('...features ingested!')\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.current_host))\n",
      "\n",
      "\u001b[37m#     feature_group = create_or_load_feature_group(\u001b[39;49;00m\n",
      "\u001b[37m#         prefix=args.feature_store_offline_prefix, feature_group_name=args.feature_group_name\u001b[39;49;00m\n",
      "\u001b[37m#     )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     feature_group.describe()\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(feature_group.as_hive_ddl())\u001b[39;49;00m\n",
      "\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    transform_tsv_to_tfrecord = functools.partial(\n",
      "        _transform_tsv_to_tfrecord,\n",
      "        max_seq_length=args.max_seq_length,\n",
      "        balance_dataset=args.balance_dataset,\n",
      "        prefix=args.feature_store_offline_prefix,\n",
      "        feature_group_name=args.feature_group_name,\n",
      "    )\n",
      "\n",
      "    input_files = glob.glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.csv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data))\n",
      "\n",
      "    num_cpus = multiprocessing.cpu_count()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_cpus))\n",
      "\n",
      "    p = multiprocessing.Pool(num_cpus)\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data))\n",
      "    dirs_output = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data))\n",
      "    dirs_output = os.listdir(train_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data))\n",
      "    dirs_output = os.listdir(validation_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data))\n",
      "    dirs_output = os.listdir(test_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "\u001b[37m#     offline_store_contents = None\u001b[39;49;00m\n",
      "\u001b[37m#     while offline_store_contents is None:\u001b[39;49;00m\n",
      "\u001b[37m#         objects_in_bucket = s3.list_objects(Bucket=bucket, Prefix=args.feature_store_offline_prefix)\u001b[39;49;00m\n",
      "\u001b[37m#         if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\u001b[39;49;00m\n",
      "\u001b[37m#             offline_store_contents = objects_in_bucket[\"Contents\"]\u001b[39;49;00m\n",
      "\u001b[37m#         else:\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Waiting for data in offline store...\\n\")\u001b[39;49;00m\n",
      "\u001b[37m#             sleep(60)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Data available.\")\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: Ryanair-Reviews-BERT-Experiment-1712883165\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "experiment = Experiment.create(\n",
    "    experiment_name=\"Ryanair-Reviews-BERT-Experiment-{}\".format(timestamp),\n",
    "    description=\"Ryanair Reviews BERT Experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "experiment_name = experiment.experiment_name\n",
    "print(\"Experiment name: {}\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1712883165\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "trial = Trial.create(\n",
    "    trial_name=\"trial-{}\".format(timestamp), experiment_name=experiment_name, sagemaker_boto_client=sm\n",
    ")\n",
    "\n",
    "trial_name = trial.trial_name\n",
    "print(\"Trial name: {}\".format(trial_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Experiment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"TrialName\": trial_name,\n",
    "    \"TrialComponentDisplayName\": \"prepare\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Store and Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto3.Session().client(service_name=\"sagemaker-featurestore-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-store-1712883165\n"
     ]
    }
   ],
   "source": [
    "timestamp = int(time.time())\n",
    "\n",
    "feature_store_offline_prefix = \"reviews-feature-store-\" + str(timestamp)\n",
    "\n",
    "print(feature_store_offline_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews-feature-group-1712883165\n"
     ]
    }
   ],
   "source": [
    "feature_group_name = \"reviews-feature-group-\" + str(timestamp)\n",
    "\n",
    "print(feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_definition import (\n",
    "    FeatureDefinition,\n",
    "    FeatureTypeEnum,\n",
    ")\n",
    "\n",
    "feature_definitions = [\n",
    "    FeatureDefinition(feature_name=\"input_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"input_mask\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"segment_ids\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label_id\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    FeatureDefinition(feature_name=\"record_id\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"date\", feature_type=FeatureTypeEnum.STRING),\n",
    "    FeatureDefinition(feature_name=\"label\", feature_type=FeatureTypeEnum.INTEGRAL),\n",
    "    #FeatureDefinition(feature_name='review', feature_type=FeatureTypeEnum.STRING)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureGroup(name='reviews-feature-group-1712883165', sagemaker_session=<sagemaker.session.Session object at 0x7f88c1b79b10>, feature_definitions=[FeatureDefinition(feature_name='input_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='input_mask', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='segment_ids', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label_id', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>), FeatureDefinition(feature_name='record_id', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='date', feature_type=<FeatureTypeEnum.STRING: 'String'>), FeatureDefinition(feature_name='label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>)])\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "feature_group = FeatureGroup(name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sess)\n",
    "\n",
    "print(feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_instance_type = \"ml.m5.large\"\n",
    "processing_instance_count = 2\n",
    "train_split_percentage = 0.50\n",
    "validation_split_percentage = 0.25\n",
    "test_split_percentage = 0.25\n",
    "balance_dataset = True\n",
    "max_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    #env={\"AWS_DEFAULT_REGION\": region},\n",
    "    max_runtime_in_seconds=7200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name sagemaker-scikit-learn-2024-04-12-00-52-45-830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2024-04-12-00-52-45-830\n",
      "Inputs:  [{'InputName': 'raw-input-data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://team-4-project-data/ryanair-data/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor.run(\n",
    "    code=\"preprocess-scikit-text-to-bert-feature-store.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"raw-input-data\",\n",
    "            source=raw_input_data_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/data/\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-train\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/bert/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-validation\",\n",
    "            s3_upload_mode=\"EndOfJob\",\n",
    "            source=\"/opt/ml/processing/output/bert/validation\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"bert-test\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/bert/test\"\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--train-split-percentage\",\n",
    "        str(train_split_percentage),\n",
    "        \"--validation-split-percentage\",\n",
    "        str(validation_split_percentage),\n",
    "        \"--test-split-percentage\",\n",
    "        str(test_split_percentage),\n",
    "        \"--max-seq-length\",\n",
    "        str(max_seq_length),\n",
    "        \"--balance-dataset\",\n",
    "        str(balance_dataset),\n",
    "        \"--feature-store-offline-prefix\",\n",
    "        str(feature_store_offline_prefix),\n",
    "        \"--feature-group-name\",\n",
    "        str(feature_group_name),\n",
    "    ],\n",
    "    experiment_config=experiment_config,\n",
    "    logs=True,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2024-04-12-00-52-45-830\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()[\"ProcessingJobName\"]\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/sagemaker-scikit-learn-2024-04-12-00-52-45-830\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(\n",
    "            region, scikit_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2024-04-12-00-52-45-830;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(\n",
    "            region, scikit_processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/team-4-project-data/sagemaker-scikit-learn-2024-04-12-00-52-45-830/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(\n",
    "            bucket, scikit_processing_job_name, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'raw-input-data', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://team-4-project-data/ryanair-data/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/input/code/preprocess-scikit-text-to-bert-feature-store.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}, 'AppManaged': False}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2024-04-12-00-52-45-830', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.large', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 7200}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'], 'ContainerArguments': ['--train-split-percentage', '0.5', '--validation-split-percentage', '0.25', '--test-split-percentage', '0.25', '--max-seq-length', '64', '--balance-dataset', 'True', '--feature-store-offline-prefix', 'reviews-feature-store-1712883165', '--feature-group-name', 'reviews-feature-group-1712883165']}, 'RoleArn': 'arn:aws:iam::211125778552:role/service-role/AmazonSageMaker-ExecutionRole-20240215T152311', 'ExperimentConfig': {'ExperimentName': 'Ryanair-Reviews-BERT-Experiment-1712883165', 'TrialName': 'trial-1712883165', 'TrialComponentDisplayName': 'prepare'}, 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/sagemaker-scikit-learn-2024-04-12-00-52-45-830', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2024, 4, 12, 0, 52, 46, 358000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2024, 4, 12, 0, 52, 46, 358000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '9c5e7058-3069-4721-b864-3ae39ad630c5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9c5e7058-3069-4721-b864-3ae39ad630c5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2878', 'date': 'Fri, 12 Apr 2024 00:52:46 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=scikit_processing_job_name, sagemaker_session=sess\n",
    ")\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................Collecting tensorflow==2.3.1\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "Collecting tensorflow==2.3.1\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "      320.4/320.4 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel>=0.26 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.37.1)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "      2.9/2.9 MB 70.4 MB/s eta 0:00:00\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "      133.7/133.7 kB 18.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (3.20.2)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "      6.0/6.0 MB 80.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.62.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "      5.6/5.6 MB 89.1 MB/s eta 0:00:00\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "      20.1/20.1 MB 47.0 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "      459.0/459.0 kB 45.5 MB/s eta 0:00:00\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "      65.5/65.5 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "      77.5/77.5 kB 14.5 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "      42.6/42.6 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "      4.9/4.9 MB 81.9 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "      94.2/94.2 kB 13.5 MB/s eta 0:00:00\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "      189.2/189.2 kB 29.1 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "      781.3/781.3 kB 60.6 MB/s eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "      233.6/233.6 kB 35.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /miniconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.28.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /miniconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (65.6.3)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "      181.3/181.3 kB 28.3 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "      320.4/320.4 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "      2.9/2.9 MB 83.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel>=0.26 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (0.37.1)\n",
      "Collecting numpy<1.19.0,>=1.16.0\n",
      "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "      20.1/20.1 MB 59.7 MB/s eta 0:00:00\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "      133.7/133.7 kB 25.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "      459.0/459.0 kB 53.8 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "      57.5/57.5 kB 11.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "      65.5/65.5 kB 13.6 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "      77.5/77.5 kB 14.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /miniconda3/lib/python3.7/site-packages (from tensorflow==2.3.1) (3.20.2)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "      42.6/42.6 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "      6.0/6.0 MB 85.8 MB/s eta 0:00:00\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /miniconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /miniconda3/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.13.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "      84.9/84.9 kB 14.6 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "      151.7/151.7 kB 19.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, wrapt, werkzeug, termcolor, tensorboard-data-server, pyasn1, oauthlib, numpy, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, markdown, keras-preprocessing, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.62.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "      5.6/5.6 MB 92.1 MB/s eta 0:00:00\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "      233.6/233.6 kB 16.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "      4.9/4.9 MB 88.9 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "      781.3/781.3 kB 57.2 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "      94.2/94.2 kB 14.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /miniconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.28.1)\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 0.15.6\n",
      "    Uninstalling Werkzeug-0.15.6:\n",
      "      Successfully uninstalled Werkzeug-0.15.6\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "      189.2/189.2 kB 31.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /miniconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (65.6.3)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "      181.3/181.3 kB 29.9 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /miniconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /miniconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /miniconda3/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.13.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "      84.9/84.9 kB 18.3 MB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "      151.7/151.7 kB 27.3 MB/s eta 0:00:00\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "Installing collected packages: tensorflow-estimator, tensorboard-plugin-wit, wrapt, werkzeug, termcolor, tensorboard-data-server, pyasn1, oauthlib, numpy, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, opt-einsum, markdown, keras-preprocessing, h5py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 0.15.6\n",
      "    Uninstalling Werkzeug-0.15.6:\n",
      "      Successfully uninstalled Werkzeug-0.15.6\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-sklearn-container 2.0 requires numpy==1.19.2, but you have numpy 1.18.5 which is incompatible.\n",
      "sagemaker-sklearn-container 2.0 requires Werkzeug==0.15.6, but you have werkzeug 2.2.3 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 gast-0.3.3 google-auth-2.29.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.1 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.4.4 numpy-1.19.2 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-2.3.0 werkzeug-2.2.3 wrapt-1.16.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2024-04-12 00:57:53.810399: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-04-12 00:57:53.810745: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-sklearn-container 2.0 requires numpy==1.19.2, but you have numpy 1.18.5 which is incompatible.\n",
      "sagemaker-sklearn-container 2.0 requires Werkzeug==0.15.6, but you have werkzeug 2.2.3 which is incompatible.\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 gast-0.3.3 google-auth-2.29.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.62.1 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.4.4 numpy-1.19.2 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-2.3.0 werkzeug-2.2.3 wrapt-1.16.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "[notice] A new release of pip is available: 23.0 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "2024-04-12 00:57:58.127571: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-04-12 00:57:58.128003: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.6.0.*, but conda is ignoring the .* and treating it as 1.6.0\n",
      "WARNING conda.models.version:get_matcher(541): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.13.0\n",
      "  latest version: 24.3.0\n",
      "Please update conda by running\n",
      "    $ conda update -n base -c defaults conda\n",
      "## Package Plan ##\n",
      "  environment location: /miniconda3\n",
      "  added / updated specs:\n",
      "    - transformers==3.5.1\n",
      "The following packages will be downloaded:\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n",
      "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
      "    click-8.0.0                |   py37h89c1867_0         144 KB  conda-forge\n",
      "    colorama-0.4.6             |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    filelock-3.13.4            |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    joblib-1.3.2               |     pyhd8ed1ab_0         216 KB  conda-forge\n",
      "    packaging-23.2             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
      "    protobuf-3.12.4            |   py37h3340039_0         707 KB  conda-forge\n",
      "    regex-2022.4.24            |   py37h540881e_0         384 KB  conda-forge\n",
      "    sacremoses-0.0.53          |     pyhd8ed1ab_0         427 KB  conda-forge\n",
      "    sentencepiece-0.1.96       |   py37h7cecad7_1         8.2 MB  conda-forge\n",
      "    tokenizers-0.9.4           |   py37h17e0dd7_1         2.6 MB  conda-forge\n",
      "    tqdm-4.66.2                |     pyhd8ed1ab_0          87 KB  conda-forge\n",
      "    transformers-3.5.1         |     pyhd8ed1ab_0         685 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        13.8 MB\n",
      "The following NEW packages will be INSTALLED:\n",
      "  click              conda-forge/linux-64::click-8.0.0-py37h89c1867_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_0\n",
      "  filelock           conda-forge/noarch::filelock-3.13.4-pyhd8ed1ab_0\n",
      "  joblib             conda-forge/noarch::joblib-1.3.2-pyhd8ed1ab_0\n",
      "  packaging          conda-forge/noarch::packaging-23.2-pyhd8ed1ab_0\n",
      "  protobuf           conda-forge/linux-64::protobuf-3.12.4-py37h3340039_0\n",
      "  regex              conda-forge/linux-64::regex-2022.4.24-py37h540881e_0\n",
      "  sacremoses         conda-forge/noarch::sacremoses-0.0.53-pyhd8ed1ab_0\n",
      "  sentencepiece      conda-forge/linux-64::sentencepiece-0.1.96-py37h7cecad7_1\n",
      "  tokenizers         conda-forge/linux-64::tokenizers-0.9.4-py37h17e0dd7_1\n",
      "  tqdm               conda-forge/noarch::tqdm-4.66.2-pyhd8ed1ab_0\n",
      "  transformers       conda-forge/noarch::transformers-3.5.1-pyhd8ed1ab_0\n",
      "The following packages will be UPDATED:\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2024.2.2-hbcca054_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "  openssl              pkgs/main::openssl-1.1.1t-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0\n",
      "Downloading and Extracting Packages\n",
      "#015ca-certificates-2024 | 152 KB    |            |   0% #015ca-certificates-2024 | 152 KB    | ########## | 100% \n",
      "#015protobuf-3.12.4      | 707 KB    |            |   0% #015protobuf-3.12.4      | 707 KB    | ########## | 100% #015protobuf-3.12.4      | 707 KB    | ########## | 100% \n",
      "#015transformers-3.5.1   | 685 KB    |            |   0% #015transformers-3.5.1   | 685 KB    | ########## | 100% #015transformers-3.5.1   | 685 KB    | ########## | 100% \n",
      "#015filelock-3.13.4      | 15 KB     |            |   0% #015filelock-3.13.4      | 15 KB     | ########## | 100% \n",
      "#015click-8.0.0          | 144 KB    |            |   0% #015click-8.0.0          | 144 KB    | ########## | 100% \n",
      "#015tqdm-4.66.2          | 87 KB     |            |   0% #015tqdm-4.66.2          | 87 KB     | ########## | 100% \n",
      "#015sentencepiece-0.1.96 | 8.2 MB    |            |   0% #015sentencepiece-0.1.96 | 8.2 MB    | ########## | 100% #015sentencepiece-0.1.96 | 8.2 MB    | ########## | 100% \n",
      "#015packaging-23.2       | 48 KB     |            |   0% #015packaging-23.2       | 48 KB     | ########## | 100% \n",
      "#015tokenizers-0.9.4     | 2.6 MB    |            |   0% #015tokenizers-0.9.4     | 2.6 MB    | ########## | 100% #015tokenizers-0.9.4     | 2.6 MB    | ########## | 100% \n",
      "#015certifi-2024.2.2     | 157 KB    |            |   0% #015certifi-2024.2.2     | 157 KB    | ########## | 100% \n",
      "#015regex-2022.4.24      | 384 KB    |            |   0% #015regex-2022.4.24      | 384 KB    | ########## | 100% #015regex-2022.4.24      | 384 KB    | ########## | 100% \n",
      "#015joblib-1.3.2         | 216 KB    |            |   0% #015joblib-1.3.2         | 216 KB    | ########## | 100% \n",
      "#015sacremoses-0.0.53    | 427 KB    |            |   0% #015sacremoses-0.0.53    | 427 KB    | ########## | 100% #015sacremoses-0.0.53    | 427 KB    | ########## | 100% \n",
      "#015colorama-0.4.6       | 25 KB     |            |   0% #015colorama-0.4.6       | 25 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Solving environment: ...working... done\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.13.0\n",
      "  latest version: 24.3.0\n",
      "Please update conda by running\n",
      "    $ conda update -n base -c defaults conda\n",
      "## Package Plan ##\n",
      "  environment location: /miniconda3\n",
      "  added / updated specs:\n",
      "    - transformers==3.5.1\n",
      "The following packages will be downloaded:\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2024.2.2   |       hbcca054_0         152 KB  conda-forge\n",
      "    certifi-2024.2.2           |     pyhd8ed1ab_0         157 KB  conda-forge\n",
      "    click-8.0.0                |   py37h89c1867_0         144 KB  conda-forge\n",
      "    colorama-0.4.6             |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    filelock-3.13.4            |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    joblib-1.3.2               |     pyhd8ed1ab_0         216 KB  conda-forge\n",
      "    packaging-23.2             |     pyhd8ed1ab_0          48 KB  conda-forge\n",
      "    protobuf-3.12.4            |   py37h3340039_0         707 KB  conda-forge\n",
      "    regex-2022.4.24            |   py37h540881e_0         384 KB  conda-forge\n",
      "    sacremoses-0.0.53          |     pyhd8ed1ab_0         427 KB  conda-forge\n",
      "    sentencepiece-0.1.96       |   py37h7cecad7_1         8.2 MB  conda-forge\n",
      "    tokenizers-0.9.4           |   py37h17e0dd7_1         2.6 MB  conda-forge\n",
      "    tqdm-4.66.2                |     pyhd8ed1ab_0          87 KB  conda-forge\n",
      "    transformers-3.5.1         |     pyhd8ed1ab_0         685 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        13.8 MB\n",
      "The following NEW packages will be INSTALLED:\n",
      "  click              conda-forge/linux-64::click-8.0.0-py37h89c1867_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_0\n",
      "  filelock           conda-forge/noarch::filelock-3.13.4-pyhd8ed1ab_0\n",
      "  joblib             conda-forge/noarch::joblib-1.3.2-pyhd8ed1ab_0\n",
      "  packaging          conda-forge/noarch::packaging-23.2-pyhd8ed1ab_0\n",
      "  protobuf           conda-forge/linux-64::protobuf-3.12.4-py37h3340039_0\n",
      "  regex              conda-forge/linux-64::regex-2022.4.24-py37h540881e_0\n",
      "  sacremoses         conda-forge/noarch::sacremoses-0.0.53-pyhd8ed1ab_0\n",
      "  sentencepiece      conda-forge/linux-64::sentencepiece-0.1.96-py37h7cecad7_1\n",
      "  tokenizers         conda-forge/linux-64::tokenizers-0.9.4-py37h17e0dd7_1\n",
      "  tqdm               conda-forge/noarch::tqdm-4.66.2-pyhd8ed1ab_0\n",
      "  transformers       conda-forge/noarch::transformers-3.5.1-pyhd8ed1ab_0\n",
      "The following packages will be UPDATED:\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2024.2.2-hbcca054_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2024.2.2-pyhd8ed1ab_0\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "  openssl              pkgs/main::openssl-1.1.1t-h7f8727e_0 --> conda-forge::openssl-1.1.1o-h166bdaf_0\n",
      "Downloading and Extracting Packages\n",
      "#015packaging-23.2       | 48 KB     |            |   0% #015packaging-23.2       | 48 KB     | ########## | 100% \n",
      "#015sentencepiece-0.1.96 | 8.2 MB    |            |   0% #015sentencepiece-0.1.96 | 8.2 MB    | #######9   |  79% #015sentencepiece-0.1.96 | 8.2 MB    | ########## | 100% \n",
      "#015click-8.0.0          | 144 KB    |            |   0% #015click-8.0.0          | 144 KB    | ########## | 100% \n",
      "#015protobuf-3.12.4      | 707 KB    |            |   0% #015protobuf-3.12.4      | 707 KB    | ########## | 100% #015protobuf-3.12.4      | 707 KB    | ########## | 100% \n",
      "#015transformers-3.5.1   | 685 KB    |            |   0% #015transformers-3.5.1   | 685 KB    | ########## | 100% #015transformers-3.5.1   | 685 KB    | ########## | 100% \n",
      "#015certifi-2024.2.2     | 157 KB    |            |   0% #015certifi-2024.2.2     | 157 KB    | ########## | 100% \n",
      "#015filelock-3.13.4      | 15 KB     |            |   0% #015filelock-3.13.4      | 15 KB     | ########## | 100% \n",
      "#015joblib-1.3.2         | 216 KB    |            |   0% #015joblib-1.3.2         | 216 KB    | ########## | 100% \n",
      "#015tqdm-4.66.2          | 87 KB     |            |   0% #015tqdm-4.66.2          | 87 KB     | ########## | 100% \n",
      "#015sacremoses-0.0.53    | 427 KB    |            |   0% #015sacremoses-0.0.53    | 427 KB    | ########## | 100% #015sacremoses-0.0.53    | 427 KB    | ########## | 100% \n",
      "#015colorama-0.4.6       | 25 KB     |            |   0% #015colorama-0.4.6       | 25 KB     | ########## | 100% \n",
      "#015ca-certificates-2024 | 152 KB    |            |   0% #015ca-certificates-2024 | 152 KB    | ########## | 100% \n",
      "#015tokenizers-0.9.4     | 2.6 MB    |            |   0% #015tokenizers-0.9.4     | 2.6 MB    | ########## | 100% #015tokenizers-0.9.4     | 2.6 MB    | ########## | 100% \n",
      "#015regex-2022.4.24      | 384 KB    |            |   0% #015regex-2022.4.24      | 384 KB    | ########## | 100% #015regex-2022.4.24      | 384 KB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|| 232k/232k [00:00<00:00, 18.4MB/s]\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=True, current_host='algo-1', feature_group_name='reviews-feature-group-1712883165', feature_store_offline_prefix='reviews-feature-store-1712883165', hosts=['algo-1', 'algo-2'], input_data='/opt/ml/processing/input/data', max_seq_length=64, output_data='/opt/ml/processing/output', test_split_percentage=0.25, train_split_percentage=0.5, validation_split_percentage=0.25)\n",
      "Environment variables:\n",
      "environ({'PATH': '/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'HOSTNAME': 'ip-10-0-70-59.ec2.internal', 'AWS_REGION': 'us-east-1', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/zAii4_h1qqiBa7BLen-2vG0GM0iWsjpZXUHkdBDMyvM', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHONUNBUFFERED': '1', 'PYTHONIOENCODING': 'UTF-8', 'LANG': 'C.UTF-8', 'LC_ALL': 'C.UTF-8', 'SAGEMAKER_SKLEARN_VERSION': '0.23-1', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_sklearn_container.training:main', 'SAGEMAKER_SERVING_MODULE': 'sagemaker_sklearn_container.serving:main', 'SKLEARN_MMS_CONFIG': '/home/model-server/config.properties', 'SM_INPUT': '/opt/ml/input', 'SM_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'SM_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json', 'SM_CHECKPOINT_CONFIG_FILE': '/opt/ml/input/config/checkpointconfig.json', 'SM_MODEL_DIR': '/opt/ml/model', 'TEMP': '/home/model-server/tmp', 'HOME': '/root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF2_BEHAVIOR': '1'})\n",
      "Current host: algo-1\n",
      "num_cpus 2\n",
      "Listing contents of /opt/ml/processing/output\n",
      "bert\n",
      "Listing contents of /opt/ml/processing/output/bert/train\n",
      "Listing contents of /opt/ml/processing/output/bert/validation\n",
      "Listing contents of /opt/ml/processing/output/bert/test\n",
      "Complete\n",
      "#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|| 232k/232k [00:00<00:00, 39.0MB/s]\n",
      "Loaded arguments:\n",
      "Namespace(balance_dataset=True, current_host='algo-2', feature_group_name='reviews-feature-group-1712883165', feature_store_offline_prefix='reviews-feature-store-1712883165', hosts=['algo-1', 'algo-2'], input_data='/opt/ml/processing/input/data', max_seq_length=64, output_data='/opt/ml/processing/output', test_split_percentage=0.25, train_split_percentage=0.5, validation_split_percentage=0.25)\n",
      "Environment variables:\n",
      "environ({'PATH': '/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'HOSTNAME': 'ip-10-0-116-178.ec2.internal', 'AWS_REGION': 'us-east-1', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/zAii4_h1qqiBa7BLen-2vG0GM0iWsjpZXUHkdBDMyvM', 'PYTHONDONTWRITEBYTECODE': '1', 'PYTHONUNBUFFERED': '1', 'PYTHONIOENCODING': 'UTF-8', 'LANG': 'C.UTF-8', 'LC_ALL': 'C.UTF-8', 'SAGEMAKER_SKLEARN_VERSION': '0.23-1', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_sklearn_container.training:main', 'SAGEMAKER_SERVING_MODULE': 'sagemaker_sklearn_container.serving:main', 'SKLEARN_MMS_CONFIG': '/home/model-server/config.properties', 'SM_INPUT': '/opt/ml/input', 'SM_INPUT_TRAINING_CONFIG_FILE': '/opt/ml/input/config/hyperparameters.json', 'SM_INPUT_DATA_CONFIG_FILE': '/opt/ml/input/config/inputdataconfig.json', 'SM_CHECKPOINT_CONFIG_FILE': '/opt/ml/input/config/checkpointconfig.json', 'SM_MODEL_DIR': '/opt/ml/model', 'TEMP': '/home/model-server/tmp', 'HOME': '/root', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF2_BEHAVIOR': '1'})\n",
      "Current host: algo-2\n",
      "num_cpus 2\n",
      "file /opt/ml/processing/input/data/ryanair_reviews.csv\n",
      "max_seq_length 64\n",
      "balance_dataset True\n",
      "prefix reviews-feature-store-1712883165\n",
      "feature_group_name reviews-feature-group-1712883165\n",
      "Shape of dataframe (2249, 21)\n",
      "Shape of balanced dataframe (570, 21)\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "4     1.0\n",
      "     ... \n",
      "95    2.0\n",
      "96    2.0\n",
      "97    2.0\n",
      "98    2.0\n",
      "99    2.0\n",
      "Name: Overall Rating, Length: 100, dtype: float64\n",
      "Shape of dataframe before splitting (570, 21)\n",
      "train split percentage 0.5\n",
      "validation split percentage 0.25\n",
      "test split percentage 0.25\n",
      "holdout percentage 0.5\n",
      "test holdout percentage 0.5\n",
      "Shape of train dataframe (285, 21)\n",
      "Shape of validation dataframe (142, 21)\n",
      "Shape of test dataframe (143, 21)\n",
      "2024-04-12T01:06:16Z\n",
      "Writing input 0 of 285\n",
      "Writing input 0 of 142\n",
      "Writing input 0 of 143\n",
      "Listing contents of /opt/ml/processing/output\n",
      "bert\n",
      "Listing contents of /opt/ml/processing/output/bert/train\n",
      "part-algo-2-ryanair_reviews.tfrecord\n",
      "Listing contents of /opt/ml/processing/output/bert/validation\n",
      "part-algo-2-ryanair_reviews.tfrecord\n",
      "Listing contents of /opt/ml/processing/output/bert/test\n",
      "part-algo-2-ryanair_reviews.tfrecord\n",
      "Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-train\n",
      "s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-validation\n",
      "s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-04-12-00-52-45-830/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "output_config = processing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"bert-train\":\n",
    "        processed_train_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bert-validation\":\n",
    "        processed_validation_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"bert-test\":\n",
    "        processed_test_data_s3_uri = output[\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 01:06:26     101848 part-algo-2-ryanair_reviews.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 01:06:26      50608 part-algo-2-ryanair_reviews.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-12 01:06:26      51071 part-algo-2-ryanair_reviews.tfrecord\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_on_aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
